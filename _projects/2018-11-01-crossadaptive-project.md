---
title: "Cross adaptive processing as musical intervention"
header: 
  teaser: /assets/img/crossadaptive-teaser.png
tags: writing
layout: single_tagged
classes: wide
---

The project explores cross-adaptive processing as a radical intervention in the communication between performing musicians. It involved development of crossadaptive processing software, new methods of musical interplay and improvisation, and philosophical discussions and reflections on the techniques as used in action.  
 
Digital audio analysis and processing techniques were used to enable features of one sound to inform the processing of another. This techniques allows the actions of one performer to directly influence another performerâ€™s sound, and doing so only by means of the acoustic signal produced by normal musical expression on the instrument.

The project method was based on iterative practical experimentation done in studio sessions. Typically two acoustic performers and one processing musician would participate in a session but work in larger groups were also explored. One or several external observers would usually participate in the sessions, as an external eye and ear. Sessions were documented by multitrack audio and video recording, and concluded with short personal video interviews with the participants. To enable the cross adaptive processing methods, a number of software tools for this kind of musical performance were developed. Sessions documentation, reflections, software and other material are available on the frozen [project blog](http://crossadaptive.hf.ntnu.no). Source code for the audio processing plugins available on giuthub as [featexmod](https://github.com/Oeyvind/featexmod) and [liveconvolver](https://github.com/Oeyvind/liveconvolver)

![image]({{ site.baseurl }}/assets/img/crossadaptive2018-1024x786.png)
*Collage of images from the project* 

Publications from the project:  
[Working Methods and Instrument Design for Cross-Adaptive Sessions](https://www.nime.org/proceedings/2018/nime2018_paper0001.pdf) by &Oslash;yvind Brandtsegg, Trond Engum & Bernt Isak W&aelig;rstad  
[Live Convolution with Time-Varying Filters](https://www.mdpi.com/2076-3417/8/1/103) by &Oslash;yvind Brandtsegg, Sigurd Saue and Victor Lazzarini  
[Applications of Cross-Adaptive Audio Effects: Automatic Mixing, Live Performance and Everything in Between](https://www.frontiersin.org/articles/10.3389/fdigh.2018.00017/full) by Joshua D. Reiss and &Oslash;yvind Brandtsegg  
[Instrumentality, perception and listening in crossadaptive performance](https://live-interfaces.github.io/2018/pdf/ICLI2018-Baalman.pdf) by Marije Baalman, Simon Emmerson and &Oslash;yvind Brandtsegg  
Music release: [Trondheim EMP: Poke It With A Stick / Joining The Bots](https://www.cronicaelectronica.org/releases/146)  


Project activites ran from 2016 to 2018 and was supported by:  
The Norwegian Programme for Artistic Research, NTNU, University of California San Diego, Maynooth University, De Montfort University, Queen Mary University of London, Norwegian Academy of Music.

